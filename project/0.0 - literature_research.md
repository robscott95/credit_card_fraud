# Why?
This file is meant to hold information about early, exploratory research into the topic of machine learning for fraud detection, with focus on data imbalance problem within it.

It will list (beside the link to the article) bulletpoints regarding highlights of the article.

### Summary: 
It looks like, based on the articles below, that we can expect an imbalanced dataset (quite logical to assume that target class, which is fraud, is quite a rare occurence across all the available data points). 
1. Check for the % of frauds in the Kaggle dataset to confirm the imbalance.
2. Pre-process the dataset. Apply any standarization and cleaning steps.
3. Use [`imblearn`](https://pypi.org/project/imblearn/) library to properly prepare the test dataset (do it in a pipeline to only affect learning sets when in CV).
    1. For starters, I'm going to go with SMOTEENN as it looks like it will clean the space out of noise more liberally.
        * If time will permit, I'll also train models with SMOTETomek and ADASYN to compare the results.
        * It's quite simple to do so (the library we're using has it all implemented under one class), and at the early stage I'm just focused on getting a good baseline model.
        * The SMOTEENN, SMOTETomek and ADASYN resampling should give us an enriched dataset.
    2. If time will permit, I'll do a test notebook which will test different methods for handling imbalanced datasets with different models and compare it to the baseline.
        * Might be a good idea to go for boosting ensemble methods (with un-resampled test-training sets too), as those will put more weight on previously misclassified labels.
        * It also sounds interesting to play around with different variants of SMOTE, ADASYN and then clean it seperately via an undersampling method like ENN.
4. Use cost sensitive learning methods on non-resampled training dataset.
    * Might not be feasible. Depends how long the 0.3a notebook will have to keep burning the CPU as it's quite a computational intensive (combining combined sampling and RandomizedSearchCV).


**Notes**: 
* Going for this two-step approach of using both re-sampled and non-resampled techniques for handling imbalanced datasets due to [this article](https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf) suggesting that both methods might be viable. In the end, I assumed, it's best to follow the evaluation metrics when making a decision.
* It might also be beneficial to look at this problem from anomaly detection perspective.
* Step 3 (that is creation of resampled datasets) MUST be inside the CV loop. [This article](https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation) explains the reason.


## Imbalanced Datasets
### Article Findings:
* I can expect imbalanced data in the EDA process - need to check for that.
    * Ideas on how to fight it:
        * Oversampling the target class
        * Undersampling the control class
        * Combine both above.
        * Use Cost Sensitive Learning methods
    * They recommended using [`imbalanced-learn`](https://pypi.org/project/imbalanced-learn/) library.
* [This article](https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9), which also is based on the dataset we're working, suggests using SMOTEENN (a mixed method utilizing first SMOTE and then edited nearest-neihgbours to clean the noisy samples from SMOTE) from the [`imbalanced-learn`](https://pypi.org/project/imbalanced-learn/) library. This is probably what we're going to do, as it will give us a nice starting point.
* It doesn't seem to be a single, definite answer on best overall methods. This implies that for given datasets there should be done some testing to determine the best approach. [Prati et al.](https://link.springer.com/chapter/10.1007/978-3-540-24694-7_32) suggests that the problem is not solely caused by class imbalance, but is related to the degree of data overlapping among classes.
* [This article](https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf) suggests, based on other articles it's citing, that cost-sensitive learning outperforms random resampling (but it looks like, [based on research](https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf), that's bound to certain domains). But clever re-sampling and combination methods can do more as they create new information or eliminate redundant information.
* The synthetic data points creation must be performed IN the cross-validation, not before. As only the cross-validated test sets should be oversampled, and the test set should remain untouched.



* Sampling based techniques [(recommended article)](https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf):
    * Undersampling of majority -> potential loss of important information. Can be random or informed (via EasyEnsemble and BalanceCascade).
    * Oversampling of minority -> can lead to overfitting as this duplicates already present observations.
        * There's of course a more "smart" way to do it. For example JOUS-Boost which introduces independently and identically distributed (iid) noise at each iteration for boosting the minority examples, for which oversampling creates replicants. It's a bit akin to Synthetic Data Generation, but much less computationally intensive.
    * Synthetic Data Generation -> A sub-tree of oversampling. Generates new data points based on already present observations.
        * SMOTE and ADASYN will create synthetic observations based on the minority.
        * ADASYN will focus on the samples which are difficult to classify with a NN rule while regular SMOTE won't make any distinction. It was based of Borderline SMOTE. It also considers how many samples should be generated based of off the distribution and how many new observations for each minority data point should be created, while SMOTE ignores that and applies equal weights.
        * SMOTE. Suspectible to generating noisy data (ADASYN can too go overboard). This can be resolved by cleaning the space after over-sampling.
            * Different SMOTE variants might generate different decision boundaries. Good idea to test it.
    * Cleaning the SMOTE based space:
        * Both methods below are available in the [`imbalanced-learn`](https://pypi.org/project/imbalanced-learn/) library.
        * Tomek's link: more restrictive than ENN. Might not clean the space as liberally.
        * Edited nearest-neighbours. This is what one of the articles that utilized our dataset suggests to go with.
    * **CBO** (cluster-based oversampling). Helps with within- and between-class imbalance by finding mean value of each class, performs a kNN and then, based on the kNN final clustering, performs oversampling of both majority and minority (up to a certain number), helping create new data points which help in representing rare concepts.
    * Sampling and Boosting combination (like SMOTEBoost, which combines both SMOTE and Adaboost, in which each ensemble classifier focuses more on the minority class).
    * Bagging (Bootstrap-aggregation) based ensemble methods can utilize various under- and oversampling techniques when using parts of the training data for building new models.
        

* Non-resampling based techniques [(recommended article)](https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf):
    * Cost Sensitive Learning (CSL):
        * Does not re-balance the sample dataset, but simply changes the cost of missclassification.
        * [Example implementation in scikit-learn for SVM](https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html)
        * There's also **MetaCost** (which sadly isn't implemented in Python libraries, but can be found Weka) which can make the base classifier cost sensitive based on the cost matrix which is created by the modeller.
        * Some of the Scikit-learn models do have class-weight param implemented, allowing to specify class-imbalance.
        * Also via the [scorer class](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) and GridSearchCV one can easily implement custom loss functions if the models allow it.
        * Boosting based ensemble methods build new classification models using misclassified observations with bigger weights.
    * **Threshold method** -> some classifiers yield a score that represents the degree to which an example is a member of class. We can use this to create several classifiers by varying the threshold of an example pertaining to class.
    * **Kernel-based learning methods**
        * Check TAF-SVM (total margin-based adaptive fuzzy svm kernel method).
        * Ignoring one of the classes. Two balancing modes are used in order to balance the data: a similarity detector (to learn a discriminator on positive examples) and a novelty detector (learn the discriminator primarily on negative examples). 
            * [Author cites an article from Lee and Cho](https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf) which suggested that regular discrimination-based inductive classifiers are suitablef or a relatively moderate imbalanced data sets.
    * **Active learning methods**  - SVM based active learning aims to select the most informative instances from the unseen training data in order to retrain the kernel-based model.
        * It's also being combined with under- and over-sampling techniques.
       


* [`imbalanced-learn`](https://pypi.org/project/imbalanced-learn/) lib has some sampling methods ready to use.
* There are some new metrics to consider when dealing with imbalanced datasets.
    * Try [Precision-Recall](https://www.nature.com/articles/nmeth.3945) and PR curve.
    
## ML models
### Summary:
Classifiers to try based on found info:
* **XGBoost**
    * One of the more popular choices when it comes to Kaggle competitions. If I had to choose one model, it would be it.
* **Random Forests** 
* SVC
* LogisticRegression + regularization

Generally, using ensemble based models should give us more robust results on the validation and test set.

We might want to standardize some of our features, especially for SVC (as it's based on distance) and our Logistic Regression (regularization requires it as it is influenced by the magnitude of coefficient). But looking on on our ensemble methods this isn't strictly necessary. Might test out non-scaled dataset on those classifiers to see how it will influence the prediction.

[This article](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) presents in a neat and tidy form for and against each standarization techniques.

Note: XGBoost implements a second algorithm, which is based on linear boosting. Scaling might influence it.
 

## Evaluation metrics:
Due to imbalanced dataset issues, I'm going to use:
* Precision - measure of exactness (how many are actually labeled correctly from all labeled as positive)
* Recall - measure of completeness (how many examples of the positive class were labeled correctly)
* F1 Score -> this will be the main metric when using RandomizedSearchCV because it balances out recall and precision harmonically
* Precision-recall curve -> will use it to evaluate top models after RandomizedSearch.
* Learning curve -> will use it to evaluate if we should tweak the model against over or underfitting.

    
## Articles:
* [Detecting Financial Fraud Using Machine Learning: Winning the War Against Imbalanced Data](https://towardsdatascience.com/detecting-financial-fraud-using-machine-learning-three-ways-of-winning-the-war-against-imbalanced-a03f8815cce9)
* [Handling imbalanced datasets in machine learning](https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28)
* [Handling imbalanced data sets in supervised learning](https://www.datasciencecentral.com/profiles/blogs/handling-imbalanced-data-sets-in-supervised-learning-using-family)
* [Handling imbalanced datasets: A review](https://pdfs.semanticscholar.org/95df/dc02010b9c390878729f459893c2a5c0898f.pdf)
* [imbalanced-learn Documentation](https://imbalanced-learn.readthedocs.io/en/stable/)
* [Fraud Detection with Cost Sensitive Learning](https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9)
* [Classification Evaluation](https://www.nature.com/articles/nmeth.3945)
* [Credit Card Fraud - Kaggle](https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets)
* [DEALING WITH IMBALANCED DATA: UNDERSAMPLING, OVERSAMPLING AND PROPER CROSS-VALIDATION](https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation)
* [Learning from Imbalanced Data](https://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf)
* [Normalization vs Standarization - quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)
* [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)
* [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)
* http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html
    